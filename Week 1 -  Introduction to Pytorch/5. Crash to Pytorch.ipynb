{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hasan\\anaconda3\\envs\\Pytorch\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import FashionMNIST # torchvision for image datasets\n",
    "from torchtext.datasets import AmazonReviewFull # torchtext for text\n",
    "from torchaudio.datasets import SPEECHCOMMANDS #torchaudio for audio\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 9, 2],\n",
       "        [3, 3, 7],\n",
       "        [1, 0, 3]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[6, 9, 2],\n",
    "     [3, 3, 7],\n",
    "     [1, 0, 3]]\n",
    "     \n",
    "A_tensor = torch.tensor(A)\n",
    "A_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([0,1,2,3])\n",
    "B_tensor = torch.from_numpy(B)\n",
    "B_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with no dtype argument, torch will infer the type\n",
    "C = torch.zeros(4,4) \n",
    "\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1]],\n",
       "\n",
       "        [[1, 1],\n",
       "         [1, 1],\n",
       "         [1, 1]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = torch.ones(3,3,2, dtype=torch.int)\n",
    "\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor_zeros = torch.zeros_like(A_tensor)\n",
    "\n",
    "A_tensor_zeros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5402, 0.7299, 0.1359],\n",
       "        [0.7315, 0.0059, 0.1360],\n",
       "        [0.0044, 0.2612, 0.4128]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dtype argument allows you to explicitly specify the datatype of the tensor\n",
    "A_tensor_rand = torch.rand_like(A_tensor, dtype=torch.float) \n",
    "\n",
    "A_tensor_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor_rand.dtype\n",
    "# torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor_rand.shape\n",
    "# torch.Size([3, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_tensor_rand.device\n",
    "# device(type='cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:17<00:00, 1527478.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 112803.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:03<00:00, 1263992.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 5184220.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = FashionMNIST(\n",
    "    # the directory you want to store the dataset, can be a string e.g. \"data\"\n",
    "    root = 'D:\\Clone Repo\\Pytorch-Self-Paced\\Week 1 -  Introduction to Pytorch\\data', \n",
    "    # if set to False, will give you the test set instead\n",
    "    train = True, \n",
    "    # download the dataset if it's not already available in the root path you specified\n",
    "    download = True, \n",
    "    # as the name implies, will transform images to tensor data structures so PyTorch can use them for training\n",
    "    transform = torchvision.transforms.ToTensor() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.classes\n",
    "# ['T-shirt/top',\n",
    "# 'Trouser',\n",
    "# 'Pullover',\n",
    "# 'Dress',\n",
    "# 'Coat',\n",
    "# 'Sandal',\n",
    "# 'Shirt',\n",
    "# 'Sneaker',\n",
    "# 'Bag',\n",
    "# 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.class_to_idx # get the corresponding index with each class\n",
    "# {'Ankle boot': 9,\n",
    "# 'Bag': 8,\n",
    "# 'Coat': 4,\n",
    "# 'Dress': 3,\n",
    "# 'Pullover': 2,\n",
    "# 'Sandal': 5,\n",
    "# 'Shirt': 6,\n",
    "# 'Sneaker': 7,\n",
    "# 'T-shirt/top': 0,\n",
    "# 'Trouser': 1}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms import Normalize, Compose\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [ToTensor(),\n",
    "     Normalize((0.5, 0.5, 0.5),  # mean\n",
    "               (0.5, 0.5, 0.5))] # std. deviation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to cifar\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [01:40<00:00, 1695966.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cifar\\cifar-10-python.tar.gz to cifar\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = CIFAR10(root=\"cifar\",\n",
    "                        train = True, # train set, 50k images\n",
    "                        download = True,\n",
    "                        transform=transform)\n",
    "test_data = CIFAR10(root = \"cifar\",\n",
    "                    train = False, # test set, 10k images\n",
    "                    download = True,\n",
    "                    transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataloader = DataLoader(training_data, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([4, 3, 32, 32])\n",
      "Shape of y: torch.Size([4]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "  print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "  print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "  break\n",
    "# Shape of X [N, C, H, W]: torch.Size([4, 3, 32, 32])\n",
    "# Shape of y: torch.Size([4]) torch.int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "  img = img / 2 + .05 # revert normalization for viewing\n",
    "  npimg = img.numpy()\n",
    "  plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "  plt.show()\n",
    "  \n",
    "classes = training_data.classes\n",
    "training_data.classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=3072, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(32*32*3, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "        \n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD( model.parameters(), lr=0.001 ) # momentum=0.9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Training Loop and Backpropagation and Training Progress Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 2000 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Testing Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304004  [    0/50000]\n",
      "loss: 2.182862  [ 8000/50000]\n",
      "loss: 2.140388  [16000/50000]\n",
      "loss: 1.477777  [24000/50000]\n",
      "loss: 1.145723  [32000/50000]\n",
      "loss: 1.666813  [40000/50000]\n",
      "loss: 1.740497  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 39.4%, Avg loss: 1.713888 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.469156  [    0/50000]\n",
      "loss: 1.433014  [ 8000/50000]\n",
      "loss: 1.353728  [16000/50000]\n",
      "loss: 1.653767  [24000/50000]\n",
      "loss: 0.909968  [32000/50000]\n",
      "loss: 0.884430  [40000/50000]\n",
      "loss: 1.382427  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 44.7%, Avg loss: 1.569790 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.937054  [    0/50000]\n",
      "loss: 1.689824  [ 8000/50000]\n",
      "loss: 1.197342  [16000/50000]\n",
      "loss: 1.910531  [24000/50000]\n",
      "loss: 0.516422  [32000/50000]\n",
      "loss: 1.482382  [40000/50000]\n",
      "loss: 1.931607  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 47.7%, Avg loss: 1.489985 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.372599  [    0/50000]\n",
      "loss: 1.033860  [ 8000/50000]\n",
      "loss: 1.011976  [16000/50000]\n",
      "loss: 1.699226  [24000/50000]\n",
      "loss: 0.857436  [32000/50000]\n",
      "loss: 2.040835  [40000/50000]\n",
      "loss: 1.736991  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 49.9%, Avg loss: 1.434511 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.037003  [    0/50000]\n",
      "loss: 0.995876  [ 8000/50000]\n",
      "loss: 1.467604  [16000/50000]\n",
      "loss: 1.055991  [24000/50000]\n",
      "loss: 2.637611  [32000/50000]\n",
      "loss: 1.119462  [40000/50000]\n",
      "loss: 1.813397  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 1.395157 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.451505  [    0/50000]\n",
      "loss: 1.107734  [ 8000/50000]\n",
      "loss: 0.927321  [16000/50000]\n",
      "loss: 1.022365  [24000/50000]\n",
      "loss: 1.097050  [32000/50000]\n",
      "loss: 1.642233  [40000/50000]\n",
      "loss: 1.280314  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Avg loss: 1.365713 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.081075  [    0/50000]\n",
      "loss: 0.817776  [ 8000/50000]\n",
      "loss: 2.489476  [16000/50000]\n",
      "loss: 0.767196  [24000/50000]\n",
      "loss: 1.142725  [32000/50000]\n",
      "loss: 1.748176  [40000/50000]\n",
      "loss: 1.243909  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 53.1%, Avg loss: 1.338418 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.144782  [    0/50000]\n",
      "loss: 0.630855  [ 8000/50000]\n",
      "loss: 2.238699  [16000/50000]\n",
      "loss: 1.158088  [24000/50000]\n",
      "loss: 2.348754  [32000/50000]\n",
      "loss: 1.464234  [40000/50000]\n",
      "loss: 0.871158  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 1.321318 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.000362  [    0/50000]\n",
      "loss: 0.999226  [ 8000/50000]\n",
      "loss: 1.306284  [16000/50000]\n",
      "loss: 1.276493  [24000/50000]\n",
      "loss: 1.408568  [32000/50000]\n",
      "loss: 0.594206  [40000/50000]\n",
      "loss: 1.136754  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 53.5%, Avg loss: 1.321107 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.102684  [    0/50000]\n",
      "loss: 0.533519  [ 8000/50000]\n",
      "loss: 2.074370  [16000/50000]\n",
      "loss: 0.371468  [24000/50000]\n",
      "loss: 1.153318  [32000/50000]\n",
      "loss: 1.091882  [40000/50000]\n",
      "loss: 0.763466  [48000/50000]\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Avg loss: 1.295089 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cifar_fc.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"cifar_fc.pth\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:  dog   dog   dog   automobile\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(test_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(make_grid(images))\n",
    "print('Ground Truth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  dog   airplane airplane automobile\n"
     ]
    }
   ],
   "source": [
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}' for j in range(4)))\n",
    "# Predicted:  dog   ship  automobile deer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 54 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "   for data in test_dataloader:\n",
    "     images, labels = data\n",
    "     outputs = model(images)\n",
    "     _, predicted = torch.max(outputs.data, 1)\n",
    "     total += labels.size(0)\n",
    "     correct += (predicted == labels).sum().item()\n",
    "     \n",
    "print(f'Model accuracy: {100 * correct // total} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifying each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class airplane: 63.0%\n",
      "Accuracy for class automobile: 67.7%\n",
      "Accuracy for class bird : 48.3%\n",
      "Accuracy for class cat  : 32.7%\n",
      "Accuracy for class deer : 48.4%\n",
      "Accuracy for class dog  : 42.7%\n",
      "Accuracy for class frog : 57.7%\n",
      "Accuracy for class horse: 60.3%\n",
      "Accuracy for class ship : 67.1%\n",
      "Accuracy for class truck: 58.6%\n"
     ]
    }
   ],
   "source": [
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0  for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "  for data in test_dataloader:\n",
    "    images, labels = data\n",
    "    outputs = model(images)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    for label,prediction in zip(labels, predictions):\n",
    "      if label == prediction:\n",
    "        correct_pred[classes[label]] += 1\n",
    "      total_pred[classes[label]] += 1\n",
    "\n",
    "for classname, correct_count in correct_pred.items():\n",
    "  accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "  print(f'Accuracy for class {classname:5s}: {accuracy:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
